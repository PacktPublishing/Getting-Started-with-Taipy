{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating input files for the forecast application\n",
    "\n",
    "This notebook creates the parquet files for the forecast application we explain in Chapter 7 of Getting Started with Taipy.\n",
    "\n",
    "**In Chapter 7, this step is optional, so if you don't have a PostgreSQL database or don't want to reproduce this step.**\n",
    "\n",
    "**This notebook's exit is a set of four parquet files. Those files are added to the repository if you don't want to reproduce this step. You can find them in `/src/data`.**\n",
    "\n",
    "First, you need to create the database. In our case, we named it `adventure_works_dw`(\"dw\" stand for \"Data Warehouse\").\n",
    "\n",
    "The files to create the tables and insert all the necessary data are in a [dedicated GitHub repository](https://github.com/enarroied/AdventureWorksSimplified). You need to run two scripts (the insert script may take a little time, like minutes maybe). And that's it.\n",
    "\n",
    "In ordet to make this notebook work, you'll need to add your credentials in the cell below. If your credentials are right and you installed all the necessary libraries from `requirements.txt`, this notebook should run fine!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters (add them here, you should not have to change any other cell in the notebook):\n",
    "\n",
    "username = \"postgres\"  # Enter your username, here we used the default \"postgres\"\n",
    "password = os.getenv(\n",
    "    \"PG_PASSWORD\"\n",
    ")  # Enter your password, we suggest using an environment variable\n",
    "host = \"localhost\"\n",
    "port = \"5432\"\n",
    "database = \"adventure_works_dw\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_connector(username, password, host, port, database):\n",
    "    # SQLAlchemy connection string\n",
    "    connection_string = (\n",
    "        f\"postgresql+psycopg2://{username}:{password}@{host}:{port}/{database}\"\n",
    "    )\n",
    "\n",
    "    engine = create_engine(connection_string)\n",
    "    return engine\n",
    "\n",
    "\n",
    "connector = create_connector(\n",
    "    username=username,\n",
    "    password=password,\n",
    "    host=host,\n",
    "    port=port,\n",
    "    database=database,\n",
    ")\n",
    "\n",
    "\n",
    "def sql_to_df(query, connector=connector):\n",
    "    df = pd.read_sql_query(query, con=connector)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data for the Sales Forecast Application\n",
    "\n",
    "First, retrive data from the database:\n",
    "\n",
    "* We only want bikes, which have a product sub-category in 1 (mountain bikes), 2 (road bikes) and 3 (touring bikes).\n",
    "* We retrieve the columns we need for the app, not more. We clean the data by trimming the columns with a `CHAR` type, and we transform the columns for better visualizations (day of week as 2-letter codes) and sub-categories as text instead of codes (*note that the original -complete_ AdventureWorksDW database has a subcategory table, here we just add a `CASE WHEN` statement to handle it*).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales = sql_to_df(\n",
    "    \"\"\"\n",
    "SELECT\n",
    "\tfulldatealternatekey AS date,\n",
    "\tCASE\n",
    "\t\tWHEN daynumberofweek = 1 THEN 'Su'\n",
    "\t\tWHEN daynumberofweek = 2 THEN 'Mo'\n",
    "\t\tWHEN daynumberofweek = 3 THEN 'Tu'\n",
    "\t\tWHEN daynumberofweek = 4 THEN 'We'\n",
    "\t\tWHEN daynumberofweek = 5 THEN 'Th'\n",
    "\t\tWHEN daynumberofweek = 6 THEN 'Fr'\n",
    "\t\tWHEN daynumberofweek = 7 THEN 'Sa'\n",
    "\t\tELSE '??'\n",
    "\tEND AS day,\n",
    "\tproductalternatekey AS product,\n",
    "\tCASE \n",
    "\t\tWHEN productsubcategorykey = 1 THEN 'Mountain'\n",
    "\t\tWHEN productsubcategorykey = 2 THEN 'Road'\n",
    "\t\tWHEN productsubcategorykey = 3 THEN 'Touring'\n",
    "\t\tELSE 'UNKNOWN'\n",
    "\tEND AS type,\n",
    "\tenglishproductname AS name,\n",
    "\tcolor AS color,\n",
    "\ttrim(style) AS style, \n",
    "\tcustomeralternatekey AS customer,\n",
    "\textract(year from birthdate) AS birth,\n",
    "\tCASE WHEN extract(year from birthdate) > 1980 THEN 'Millenial'\n",
    "\t\tWHEN extract(year from birthdate) BETWEEN 1966 AND 1980 THEN 'Gen X'\n",
    "        WHEN extract(year from birthdate) BETWEEN 1945 AND 1965 THEN 'Boomers'\n",
    "        WHEN extract(year from birthdate) < 1945 THEN 'Silent'\n",
    "        ELSE 'Unknown'\n",
    "    END AS generation,\n",
    "\tgender AS gender,\n",
    "\tunitprice AS unit_price,\n",
    "\torderquantity AS items,\n",
    "\tunitprice * orderquantity AS sales\n",
    "FROM\n",
    "    factinternetsales\n",
    "\tJOIN dimproduct ON dimproduct.productkey = factinternetsales.productkey\n",
    "\tJOIN dimdate on  dimdate.datekey = factinternetsales.orderdatekey\n",
    "\tJOIN dimcustomer on dimcustomer.customerkey = factinternetsales.customerkey\n",
    "WHERE \n",
    "\tproductsubcategorykey IN (1, 2, 3)\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales.sample(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame size before type transformation\n",
    "print(\n",
    "    f\"DataFrame size before changing data types: {round(df_sales.memory_usage(deep=True).sum()/(1024*1024), 2)} Mb\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales = df_sales.astype(\n",
    "    {\n",
    "        \"date\": \"datetime64[ns]\",\n",
    "        \"day\": \"category\",\n",
    "        \"product\": \"category\",\n",
    "        \"type\": \"category\",\n",
    "        \"name\": str,\n",
    "        \"color\": \"category\",\n",
    "        \"style\": \"category\",\n",
    "        \"customer\": \"category\",\n",
    "        \"birth\": \"int\",\n",
    "        \"generation\": str,\n",
    "        \"gender\": \"category\",\n",
    "        \"unit_price\": \"float\",\n",
    "        \"items\": \"int\",\n",
    "        \"sales\": \"float\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"DataFrame size after changing data types: {round(df_sales.memory_usage(deep=True).sum()/(1024*1024), 2)} Mb\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explorations\n",
    "\n",
    "We can plot some of the data to see how it's distributed, and look for anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales[df_sales[\"sales\"] < 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales.isnull().any().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't have negative sales, and we don't have missing values, this dataset is rather clean.\n",
    "\n",
    "Let's look at sales evolution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_by_dimensions_and_facts(df, dimension_columns, orderby=\"sales\"):\n",
    "    \"\"\"\n",
    "    Groups a DataFrame by specified dimension columns, always aggregating:\n",
    "        - Count by the \"quantity\" column.\n",
    "        - Sum the \"total_sale\" column.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame to group.\n",
    "        dimension_columns (list of str): List of column names to group by (dimensions).\n",
    "        orderby (str | list of str): columns name or list of columns to order the DataFrame. Defaults to \"sales\"\n",
    "\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A grouped DataFrame with the sum of \"total_sale\" and count of \"quantity\" for each combination of dimensions.\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    df_grouped = df_copy.groupby(dimension_columns, observed=True).agg(\n",
    "        sales=(\"sales\", \"sum\"), items=(\"items\", \"count\")\n",
    "    )\n",
    "\n",
    "    df_grouped = df_grouped.sort_values(by=orderby, ascending=False)\n",
    "\n",
    "    # Format and return the DataFrame\n",
    "    df_grouped = df_grouped.round(2)\n",
    "    df_grouped = df_grouped.reset_index()\n",
    "    return df_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales_by_date = group_by_dimensions_and_facts(df_sales, [\"date\"], orderby=\"date\")\n",
    "df_sales_by_date.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales_by_date.plot(x=\"date\", y=\"sales\", kind=\"scatter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a simplified DataFrame\n",
    "\n",
    "By creating a smaller DataFrame, we can increase the efficiency of the application.\n",
    "\n",
    "We'll still use the bigger DataFrame to display data, in a table that doesn't update. But we can create a second subset, with less columns and some level of pre-aggregation to reduce the required effort to aggreate data.\n",
    "\n",
    "We remove the following columns, that we won't use for aggregation:\n",
    "\n",
    "* product (the product ID, we'll use the \"name\", it's less efficient, but better for end users).\n",
    "* customer (the customer ID).\n",
    "* birth (we won't aggregate by birthdate -not enough data-, but by \"generation\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales_simplified = group_by_dimensions_and_facts(\n",
    "    df_sales,\n",
    "    [\n",
    "        \"date\",\n",
    "        \"day\",\n",
    "        \"type\",\n",
    "        \"name\",\n",
    "        \"color\",\n",
    "        \"style\",\n",
    "        \"generation\",\n",
    "        \"gender\",\n",
    "        \"unit_price\",\n",
    "    ],\n",
    ")\n",
    "df_sales_simplified.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_sales_simplified)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate by customer type\n",
    "\n",
    "This will only be used for table display, we'll use the simplified DataFrame for charts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales_by_customer = group_by_dimensions_and_facts(\n",
    "    df_sales, [\"customer\", \"birth\", \"generation\", \"gender\"]\n",
    ")\n",
    "\n",
    "\n",
    "df_sales_by_customer.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate by product type\n",
    "\n",
    "This will only be used for table display, we'll use the simplified DataFrame for charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales_by_product = group_by_dimensions_and_facts(\n",
    "    df_sales,\n",
    "    [\n",
    "        \"product\",\n",
    "        \"name\",\n",
    "        \"type\",\n",
    "        \"color\",\n",
    "        \"style\",\n",
    "        \"unit_price\",\n",
    "    ],\n",
    ")\n",
    "# We use the maximum unit_price value\n",
    "df_sales_by_product = (\n",
    "    df_sales_by_product.groupby(\n",
    "        [\"product\", \"name\", \"type\", \"color\", \"style\"], observed=True\n",
    "    )\n",
    "    .agg({\"unit_price\": \"max\", \"sales\": \"sum\", \"items\": \"sum\"})\n",
    "    .sort_values(by=\"sales\", ascending=False)\n",
    "    .round(2)\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "df_sales_by_product.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save DataFrames\n",
    "\n",
    "We save each DataFrame:\n",
    "\n",
    "* As CSV, for reference and so we can have a quick way to inspect data manually when we want to debug or as we build our dashboard, or to check some data with our clients.\n",
    "* As a parquet file for our application, it's a more efficient format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save csv files\n",
    "\n",
    "df_sales.to_csv(\"./csv_files/sales.csv\", index=False)\n",
    "df_sales_simplified.to_csv(\"./csv_files/sales_simplified.csv\", index=False)\n",
    "df_sales_by_customer.to_csv(\"./csv_files/sales_by_customer.csv\", index=False)\n",
    "df_sales_by_product.to_csv(\"./csv_files/sales_by_product.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"../src/data\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales.to_parquet(\"../src/data/sales.parquet\", index=False)\n",
    "df_sales_simplified.to_parquet(\"../src/data/sales_simplified.parquet\", index=False)\n",
    "df_sales_by_customer.to_parquet(\"../src/data/sales_by_customer.parquet\", index=False)\n",
    "df_sales_by_product.to_parquet(\"../src/data/sales_by_product.parquet\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ch7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

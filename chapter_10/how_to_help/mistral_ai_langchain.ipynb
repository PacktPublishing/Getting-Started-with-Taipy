{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4601845",
   "metadata": {},
   "source": [
    "\n",
    "# Using LangChain and Mistral AI’s API\n",
    "\n",
    "LangChain is a Python library that simplifies building LLM-powered applications by chaining components together—handling prompts, memory, agents, and integrations seamlessly.\n",
    "\n",
    "[Mistral AI](https://mistral.ai/) is a cutting-edge AI company specializing in open-weight models, offering an API with a free tier, perfect for testing and prototyping.\n",
    "\n",
    "Hugging Face provides an open platform for AI models, datasets, and pipelines, making it easy to integrate thousands of pre-trained models into your projects.\n",
    "\n",
    "LangChain’s modular design lets you start with Mistral’s free API and later swap in models from OpenAI, Google, Anthropic—or even Hugging Face’s vast library!\n",
    "\n",
    "## Using Mistral AI with LangChain\n",
    "\n",
    "To test our aplications, you'll need to create a Mistral AI and Hugging Face account, they're completely free.\n",
    "\n",
    "**Note that if you're already familiar with LangChain and AI models, and want to you use models your're comfortable with, you can adapt the Taipy application and ignore this notebook**.\n",
    "\n",
    "### Creating a MistraiAI account\n",
    "\n",
    "To create an account, go to: https://auth.mistral.ai/ui/ --> The process is classic and intuitive.\n",
    "\n",
    "next, go to \"Try the API\" and select the free plan (or any other plan!)\n",
    "\n",
    "![](../img/mistral_pricing.png)\n",
    "\n",
    "\n",
    "### Mistral + LangChain\n",
    "\n",
    "Mistral offers its own [Python client](https://pypi.org/project/mistralai/), but we'll use LangChain in the chapter, since it's a popular library with lots of integrations.\n",
    "\n",
    "LangChain has specific libraries lbraries to connect to thid party models, so you'll need to install both `langchain` and `langchain_mistralai`:\n",
    "\n",
    "```\n",
    "pip install langchain\n",
    "pip install langchain_mistralai\n",
    "```\n",
    "\n",
    "Langchain offers 2 powerfull abstractions to deal with chat messages: `HumanMessage` and `AIMessage`. These classes hold, respectively, the message that users send to the AI API, and the response, but they also hold metadata about the message and the ongoing interaction.\n",
    "\n",
    "The `langchain_mistralai` library has a `ChatMistralAI` class, that holds the connector to the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4d2abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_mistral_small_name = \"mistral-small-latest\"\n",
    "\n",
    "# chat_mistral_small = ChatMistralAI(model=chat_mistral_small_name, api_key=\"YOU_API_KEY\")\n",
    "chat_mistral_small = ChatMistralAI(model=chat_mistral_small_name)\n",
    "message_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801d7792",
   "metadata": {},
   "outputs": [],
   "source": [
    "def talk_to_bot(input_message: str, history: list, chat: ChatMistralAI) -> AIMessage:\n",
    "    \"\"\"Processes a user input through a LangChain chat model and updates conversation history.\n",
    "\n",
    "    Takes a user message, sends it to the specified LangChain chat model (e.g., Mistral, OpenAI),\n",
    "    appends both the input and model response to the conversation history, and returns the AI's reply.\n",
    "\n",
    "    Args:\n",
    "        input_message: The user's input text to send to the chat model.\n",
    "        history: A list of previous `HumanMessage`/`AIMessage` objects representing the conversation history.\n",
    "        chat: A LangChain `ChatMistralAI` or chat model instance configured for message processing.\n",
    "\n",
    "    Returns:\n",
    "        AIMessage: The chat model's response object containing content and metadata.\n",
    "\n",
    "    Example:\n",
    "        >>> history = []\n",
    "        >>> response = talk_to_bot(\"Hello!\", history, chat_chain)\n",
    "        >>> print(response.content)\n",
    "        \"Hi there! How can I help you today?\"\n",
    "    \"\"\"\n",
    "    history.append(HumanMessage(content=input_message))\n",
    "    response = chat.invoke(history)\n",
    "    history.append(response)\n",
    "    return response, history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f84031",
   "metadata": {},
   "source": [
    "Now we can use the `talk_to_bot()` function, passing a message for the LLM, an empty list for the message hitory, and specifying a chat model. The response is an `AIMessage` object, with a `.content` attribute, that holds the model's response, and some extra metadata attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92be8463",
   "metadata": {},
   "outputs": [],
   "source": [
    "response, history = talk_to_bot(\n",
    "    \"Hi! What is the capital of Andorra?\", message_history, chat_mistral_small\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2338d594",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0c8dc5",
   "metadata": {},
   "source": [
    "The `message_history` object now holds the question and the answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f86b490",
   "metadata": {},
   "outputs": [],
   "source": [
    "message_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680f8105",
   "metadata": {},
   "source": [
    "And we can continue our conversation, as long as we provide the same history, and as long as we don't exceed the maximumn amount of **tokens** for a conversation. In the context of AI and LLMs, a token can represent a single character, a word, or even part of a word, depending on the language and tokenization method used. You can see more information about tokenization in [Mistral's documentation](https://docs.mistral.ai/guides/tokenization/). Each model has a maximum amount of tokens, this is true for all models, and is usually well documented (in the case of  [Mistral](https://docs.mistral.ai/getting-started/models/models_overview/), the `mistral-small` model has 131 000 tokens for a whole conversation).\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52982d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "response, message_history = talk_to_bot(\n",
    "    \"That was interesting! And what is the capital of Liechtenstein?\",\n",
    "    message_history,\n",
    "    chat_mistral_small,\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b68c9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "message_history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mistral_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

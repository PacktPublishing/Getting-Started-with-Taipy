{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db7a5240",
   "metadata": {},
   "source": [
    "# Using Ollama to run models locally\n",
    "\n",
    "⚠️ **WARNING:** Running models locally requires downloading files that can be somehow big. It also requires either a GPU, or patience if you want to run it on RAM. This method is great for educational and hoobbying purposes, or if you have a good infrastructure and consider running local models in your company, within a production server with sufficient hardware requirements.\n",
    "\n",
    "To use the models, you need to [install Ollama first](https://ollama.com/download), it's straightforward, for all OS. \n",
    "\n",
    "Then you need to install the langchain-ollama library:\n",
    "\n",
    "```\n",
    "https://pypi.org/project/langchain-ollama/\n",
    "```\n",
    "\n",
    "Next, [select your model](https://ollama.com/search), for our example, we'll focus on lightweight models, to make it accessible to more people, but feel free to explore other models, especially if your computer has sufficient resources. Let's work with [smollm2](https://ollama.com/library/smollm2), a model that's thought for running on personal devices.\n",
    "\n",
    "To dowload the model, open your terminal an type:\n",
    "\n",
    "```bash\n",
    "ollama pull smollm2\n",
    "```\n",
    "\n",
    "Or even smaller:\n",
    "```\n",
    "ollama pull smollm2:135m\n",
    "```\n",
    "\n",
    "⚠️ **WARNING:** Small Language Models (like `smollm2:135m`) can give very strange answers, they're usually not great for production purposes, but they can be good for testing, and well, depending on your task, you may find them a good use!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4622eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0d663e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"smollm2:135m\",  # Replace with your preferred model, but ollama pull it first!\n",
    "    temperature=0.5,\n",
    "    num_gpu=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dacf3c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Here's a simple recipe for spinach:\n",
      "\n",
      "1. **Dried Spinach**: You can buy dried spinach in the grocery store or make your own at home using fresh spinach leaves.\n",
      "\n",
      "2. **Cooking Liquid Ingredients**: In a large pot, bring 3 cups of water to a boil over high heat. Add half a cup of chopped spinach and cook until the spinach is wilted, about 5-7 minutes. Remove from heat and let it cool for a minute before serving.\n",
      "\n",
      "3. **Serve with Fresh Spinach**: Drizzle some fresh spinach on top of your dish or use it as a side dish to add color to your meals.\n",
      "\n",
      "4. **Nutritional Information:** A cup of cooked spinach contains about 20 calories, 1 gram of protein (peanuts are an excellent source), and 5 grams of fiber.\n",
      "\n",
      "Enjoy cooking with spinach!\n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke(\"Make a recipe with spinach\")\n",
    "print(response.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mistral_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
